
%% bare_conf.tex
%% V1.3
%% 2007/01/11
%% by Michael Shell
%% See:
%% http://www.michaelshell.org/
%% for current contact information.
%%
%% This is a skeleton file demonstrating the use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.7 or later) with an IEEE conference paper.
%%
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/tex-archive/macros/latex/contrib/IEEEtran/
%% and
%% http://www.ieee.org/

%%*************************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or
%% implied; without even the implied warranty of MERCHANTABILITY or
%% FITNESS FOR A PARTICULAR PURPOSE!
%% User assumes all risk.
%% In no event shall IEEE or any contributor to this code be liable for
%% any damages or losses, including, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% All comments are the opinions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%
%% This work is distributed under the LaTeX Project Public License (LPPL)
%% ( http://www.latex-project.org/ ) version 1.3, and may be freely used,
%% distributed and modified. A copy of the LPPL, version 1.3, is included
%% in the base LaTeX documentation of all distributions of LaTeX released
%% 2003/12/01 or later.
%% Retain all contribution notices and credits.
%% ** Modified files should be clearly indicated as such, including  **
%% ** renaming them and changing author support contact information. **
%%
%% File list of work: IEEEtran.cls, IEEEtran_HOWTO.pdf, bare_adv.tex,
%%                    bare_conf.tex, bare_jrnl.tex, bare_jrnl_compsoc.tex
%%*************************************************************************

% *** Authors should verify (and, if needed, correct) their LaTeX system  ***
% *** with the testflow diagnostic prior to trusting their LaTeX platform ***
% *** with production work. IEEE's font choices can trigger bugs that do  ***
% *** not appear when using other class files.                            ***
% The testflow support page is at:
% http://www.michaelshell.org/tex/testflow/



% Note that the a4paper option is mainly intended so that authors in
% countries using A4 can easily print to A4 and see how their papers will
% look in print - the typesetting of the document will not typically be
% affected with changes in paper size (but the bottom and side margins will).
% Use the testflow package mentioned above to verify correct handling of
% both paper sizes by the user's LaTeX system.
%
% Also note that the "draftcls" or "draftclsnofoot", not "draft", option
% should be used if it is desired that the figures are to be displayed in
% draft mode.
%
\documentclass[10pt, conference, compsocconf]{IEEEtran}
% disable footnote numbering
\renewcommand{\thefootnote}{}
% algorithm support
%\usepackage{algorithm}
%\usepackage{algorithmicx}
%\usepackage{algpseudocode}
% add algo input & output support
%\renewcommand{\algorithmicrequire}{\textbf{Input:}}
%\renewcommand{\algorithmicensure}{\textbf{Output:}}
% add algo2e support
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
% add split equation support
\usepackage{amsmath}
% set default eps pics path
\usepackage[pdftex]{graphicx}
% default pic path
\graphicspath{{pics/}}
% pdflatex eps support
%\usepackage{epstopdf}
% single col picture and table support in two column env
\makeatletter
\newenvironment{tablehere}
  {\def\@captype{table}}
  {}

\newenvironment{figurehere}
  {\def\@captype{figure}}
  {}
\makeatother
% add small equation support
\newenvironment{sequation}{\begin{equation}\small}{\end{equation}}
% add normal size equation support
% \newenvironment{nequation}{\begin{equation}\normalsize}{\end{equation}}
% enable spacing
\usepackage{setspace}
\usepackage{subfigure}

% Add the compsocconf option for Computer Society conferences.
%
% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:
% \documentclass[conference]{../sty/IEEEtran}





% Some very useful LaTeX packages include:
% (uncomment the ones you want to load)


% *** MISC UTILITY PACKAGES ***
%
%\usepackage{ifpdf}
% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional
% compilation based on whether the output is pdf or dvi.
% usage:
% \ifpdf
%   % pdf code
% \else
%   % dvi code
% \fi
% The latest version of ifpdf.sty can be obtained from:
% http://www.ctan.org/tex-archive/macros/latex/contrib/oberdiek/
% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.






% *** CITATION PACKAGES ***
%
%\usepackage{cite}
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 4.0 (2003-05-27) and later if using hyperref.sty. cite.sty does
% not currently provide for hyperlinked citations.
% The latest version can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/cite/
% The documentation is contained in the cite.sty file itself.






% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
  % \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation can
% be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/required/graphics/
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found as epslatex.ps or
% epslatex.pdf at: http://www.ctan.org/tex-archive/info/
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex





% *** MATH PACKAGES ***
%
%\usepackage[cmex10]{amsmath}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics. If using
% it, be sure to load this package with the cmex10 option to ensure that
% only type 1 fonts will utilized at all point sizes. Without this option,
% it is possible that some math symbols, particularly those within
% footnotes, will be rendered in bitmap form which will result in a
% document that can not be IEEE Xplore compliant!
%
% Also, note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
%\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/required/amslatex/math/





% *** SPECIALIZED LIST PACKAGES ***
%
%\usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/algorithms/
% There is also a support site at:
% http://algorithms.berlios.de/index.html
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/tex-archive/macros/latex/contrib/algorithmicx/




% *** ALIGNMENT PACKAGES ***
%
%\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/required/tools/


%\usepackage{mdwmath}
%\usepackage{mdwtab}
% Also highly recommended is Mark Wooding's extremely powerful MDW tools,
% especially mdwmath.sty and mdwtab.sty which are used to format equations
% and tables, respectively. The MDWtools set is already installed on most
% LaTeX systems. The lastest version and documentation is available at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/mdwtools/


% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.


%\usepackage{eqparbox}
% Also of notable interest is Scott Pakin's eqparbox package for creating
% (automatically sized) equal width boxes - aka "natural width parboxes".
% Available at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/eqparbox/





% *** SUBFIGURE PACKAGES ***
%\usepackage[tight,footnotesize]{subfigure}
% subfigure.sty was written by Steven Douglas Cochran. This package makes it
% easy to put subfigures in your figures. e.g., "Figure 1a and 1b". For IEEE
% work, it is a good idea to load it with the tight package option to reduce
% the amount of white space around the subfigures. subfigure.sty is already
% installed on most LaTeX systems. The latest version and documentation can
% be obtained at:
% http://www.ctan.org/tex-archive/obsolete/macros/latex/contrib/subfigure/
% subfigure.sty has been superceeded by subfig.sty.



%\usepackage[caption=false]{caption}
%\usepackage[font=footnotesize]{subfig}
% subfig.sty, also written by Steven Douglas Cochran, is the modern
% replacement for subfigure.sty. However, subfig.sty requires and
% automatically loads Axel Sommerfeldt's caption.sty which will override
% IEEEtran.cls handling of captions and this will result in nonIEEE style
% figure/table captions. To prevent this problem, be sure and preload
% caption.sty with its "caption=false" package option. This is will preserve
% IEEEtran.cls handing of captions. Version 1.3 (2005/06/28) and later
% (recommended due to many improvements over 1.2) of subfig.sty supports
% the caption=false option directly:
%\usepackage[caption=false,font=footnotesize]{subfig}
%
% The latest version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/subfig/
% The latest version and documentation of caption.sty can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/caption/




% *** FLOAT PACKAGES ***
%
%\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure. The latest version and documentation can be found at:
% http://www.ctan.org/tex-archive/macros/latex/base/



%\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "\begin{figure*}[!b]" is not normally possible in
% LaTeX2e). It also provides a command:
%\fnbelowfloat
% to enable the placement of footnotes below bottom floats (the standard
% LaTeX2e kernel puts them above bottom floats). This is an invasive package
% which rewrites many portions of the LaTeX2e float routines. It may not work
% with other packages that modify the LaTeX2e float routines. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/sttools/
% Documentation is contained in the stfloats.sty comments as well as in the
% presfull.pdf file. Do not use the stfloats baselinefloat ability as IEEE
% does not allow \baselineskip to stretch. Authors submitting work to the
% IEEE should note that IEEE rarely uses double column equations and
% that authors should try to avoid such use. Do not be tempted to use the
% cuted.sty or midfloat.sty packages (also by Sigitas Tolusis) as IEEE does
% not format its papers in such ways.





% *** PDF, URL AND HYPERLINK PACKAGES ***
%
%\usepackage{url}
% url.sty was written by Donald Arseneau. It provides better support for
% handling and breaking URLs. url.sty is already installed on most LaTeX
% systems. The latest version can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/misc/
% Read the url.sty source comments for usage information. Basically,
% \url{my_url_here}.





% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
% There should be no need to do such things with IEEEtran.cls V1.6 and later.
% (Unless specifically asked to do so by the journal or conference you plan
% to submit to, of course. )


% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}
%
% paper title
% can use linebreaks \\ within to get better formatting as desired
\title{High-Responsive Scheduling with MapReduce \\Performance Prediction on Hadoop YARN}


% author names and affiliations
% use a multiple column layout for up to two different
% affiliations

\author{\IEEEauthorblockN{Yang Liu\IEEEauthorrefmark{1}, Yukun Zeng\IEEEauthorrefmark{1}, Xuefeng Piao\IEEEauthorrefmark{2}}
\IEEEauthorblockA{School of Computer Science and Technology\\
Harbin Institute of Technology\\
Weihai, China\\
Email: \IEEEauthorrefmark{1}\{yang.liu.hitwh, yukun.zeng.hit\}@gmail.com\\
\IEEEauthorrefmark{2}hbpark@hit.edu.cn
}
%\and
%\IEEEauthorblockN{Authors Name/s per 2nd Affiliation (Author)}
%\IEEEauthorblockA{line 1 (of Affiliation): dept. name of organization\\
%line 2: name of organization, acronyms acceptable\\
%line 3: City, Country\\
%line 4: Email: name@xyz.com}
}

% conference papers do not typically use \thanks and this command
% is locked out in conference mode. If really needed, such as for
% the acknowledgment of grants, issue a \IEEEoverridecommandlockouts
% after \documentclass

% for over three affiliations, or if they all won't fit within the width
% of the page, use this alternative format:
%
%\author{\IEEEauthorblockN{Michael Shell\IEEEauthorrefmark{1},
%Homer Simpson\IEEEauthorrefmark{2},
%James Kirk\IEEEauthorrefmark{3},
%Montgomery Scott\IEEEauthorrefmark{3} and
%Eldon Tyrell\IEEEauthorrefmark{4}}
%\IEEEauthorblockA{\IEEEauthorrefmark{1}School of Electrical and Computer Engineering\\
%Georgia Institute of Technology,
%Atlanta, Georgia 30332--0250\\ Email: see http://www.michaelshell.org/contact.html}
%\IEEEauthorblockA{\IEEEauthorrefmark{2}Twentieth Century Fox, Springfield, USA\\
%Email: homer@thesimpsons.com}
%\IEEEauthorblockA{\IEEEauthorrefmark{3}Starfleet Academy, San Francisco, California 96678-2391\\
%Telephone: (800) 555--1212, Fax: (888) 555--1212}
%\IEEEauthorblockA{\IEEEauthorrefmark{4}Tyrell Inc., 123 Replicant Street, Los Angeles, California 90210--4321}}




% use for special paper notices
%\IEEEspecialpapernotice{(Invited Paper)}




% make the title area
\maketitle


\begin{abstract}
Hadoop is an open-source big data analysis platform that is widely used in both academia and industry. Decoupling of resource management and programming framework, the next generation of Hadoop, namely Hadoop YARN, is accommodated to various programming frameworks and capable of handling more kinds of workload, such as interactive analysis and stream processing. However, most existent schedulers in YARN are designed for batch processing and they do not value per-job response time, which results in low responsiveness of the Hadoop platform. This paper proposes a FSPY (Fair Sojourn Protocol in YARN) scheduler to improve responsiveness with guaranteeing fairness. FSPY relies on job sizes which are unknown a priori. Consequently, we also present a job size prediction mechanism for MapReduce. Experimental results show that our scheduler outperforms Fair scheduler by 10x with respect to responsiveness under heavy workloads. Meanwhile, our prediction mechanism reaches an $R^2$ prediction accuracy of 0.97.

\end{abstract}

\begin{IEEEkeywords}
YARN; scheduling; responsiveness; fairness; job size prediction

\end{IEEEkeywords}


% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle
\footnote{\IEEEauthorrefmark{2}Xuefeng Piao is corresponding author}


\section{Introduction}
% no \IEEEPARstart
As state-of-the-art open-source big data processing platform, Apache Hadoop is now widely accepted not only in the academia, but also among real IT industries. The development of Hadoop goes through two phases: Hadoop 1.0 and Hadoop 2.0. In Hadoop 1.0, MapReduce \cite{dean2008mapreduce} is the only supported programming framework and it is tightly coupled with the resource management scheme. In Hadoop 2.0, YARN \cite{Vavilapalli2013} is introduced to independently manage computing resource of the Hadoop platform. It effectively decouples resource management and programming frameworks, which enables jobs of various programming frameworks to run simultaneously on a single cluster.

With the diversification of jobs on Hadoop, more and more concerns have been raised. One such concern is the system responsiveness which is important for both interactive analysis \cite{heer2012interactive} and stream processing \cite{zikopoulos2011understanding} applications. Under the scenario of interactive analysis, users have to wait for the results after job submission, thus the responsiveness will significantly influence user experience. Interactive analysis frameworks that have been ported onto YARN include Tez \cite{saha2015apache}, Spark \cite{zaharia2010spark} and Drill \cite{hausenblas2013apache}. Under the scenario of stream processing, large amount of stream data must be processed in real time. An example of stream data processing is real time financial fraud detection, in which a stream of transaction data is processed to detect fraudulent activities. One of the most famous stream processing frameworks is Storm \cite{iqbal2015big}, which has been ported onto YARN.

Existing resource scheduling strategies in YARN are mainly designed for batch processing systems, which results in low responsiveness. Native schedulers in YARN include:

\begin{itemize}
  \item FIFO scheduler: The scheduler assigns higher priority to earlier-submitted jobs. It performs poor in terms of responsiveness, since small jobs submitted after a large job have to wait until the large one releases the resources.
  \item Fair scheduler: The scheduler assigns each job a concurrent fair share of cluster resources. It might also cause responsiveness problems when several large jobs are running in parallel.
  \item Capacity scheduler: The scheduler arranges jobs in several queues and allocates computing resources to each queue according to specified proportion. For jobs in a Inside a queue, queue,both FIFO and Fair scheduling could be applied.
\end{itemize}

To fulfill the growing demands of responsiveness in big data applications, it's necessary to develop new scheduling policies for Hadoop YARN. Related works show that size-based scheduling policies are commonly effective to improve the system responsiveness. As a typical size-based scheduler, SRPT (Shortest Remaining Processing Time) \cite{schrage1966queue} assigns the highest priority to jobs with the least remaining processing time. SRPT is optimal in terms of average response time in single server environment \cite{bansal2001analysis}, and it is also efficient in multi-server environment \cite{harchol1998case}. 
%and has been applied to the scheduling of MapReduce jobs in \cite{guo2011automatic}, \cite{lin2013joint}, \cite{moseley2011scheduling}.

Though SRPT can minimize the average response time, it penalizes large jobs in order to accelerate the completion of small ones, which can lead to starvation of large jobs. To solve this problem, another size-based scheduler FSP (Fair Sojourn Protocol) has been proposed.  FSP assigns the highest priority to the job that would be completed first by processor sharing scheduler. According to the policy of processor sharing, jobs with smaller remaining size will finish earlier. Therefore, the FSP policy can also be stated that jobs with smaller virtual size will be assigned higher priority. Here the virtual size refers to the job remaining size in the processor sharing scheduler. Assuming that job virtual sizes at each time point are known exactly, the sojourn time (i.e., the response time) of each job in FSP shall be no longer than that in processor sharing. In this sense, FSP effectively guarantees the fairness. Furthermore, FSP is near optimal in responsiveness since the average response time of jobs under its scheduling is similar to that under SRPT.

To improve responsiveness, HFSP \cite{pastorelli2015hfsp} has been proposed to schedule MapReduce jobs in Hadoop 1.0 based on the principle of FSP. In HFSP, a job size prediction module and a simulated cluster were presented to obtain job virtual sizes at each time point. The job size prediction module can only estimate running phase size of a job. In other words, if a job is running in the map phase, the module cannot estimate its reduce-phase size or total size. The simulated cluster is to track job virtual sizes by simulating a Hadoop cluster running under Fair scheduler (a variant of processor sharing for Hadoop). However, this module is designed exclusively for resource management mechanism in Hadoop 1.0.

Thus, applying the HFSP policy to YARN raises a number of challenges. Firstly, scheduling based on job phase sizes is not efficient on YARN. The resource management mechanism in Hadoop 1.0 is based on static slot partition, in which map slots and reduce slots are separated. Consequently, there is no resource contention between map tasks and reduce tasks. However, in YARN, the static slot partition is abandoned, and resource competitions exist between tasks of different phases. As a result, scheduling based on job phase size can lead to flapping, i.e., jobs get priority alternately, which can bring down the scheduling performance. Secondly, the simulated cluster is not workable on YARN. In the simulated cluster, the scheduler usually need to foreknow the resource requests of a job whose virtual progress is ahead of its real progress. This is feasible in Hadoop 1.0, because job execution plans are accessible to the central scheduler. But in YARN, the job execution plan is managed by its own master process and not accessible to the central scheduler, which leads to the simulated cluster strategy of HFSP is no longer feasible in YARN.

To improve responsiveness for Hadoop YARN, we preserve some core ideas in FSP but make significant improvements on job size estimation, job virtual size calculation and YARN support. In our work, we present a new mechanism to predict MapReduce job sizes on YARN; we propose a light-weight job virtual size calculation approach that is well-adapted to YARN; we present a scheduling framework based on above works to improve responsiveness and guarantee fairness of Hadoop YARN.

The rest of this paper is organized as follows. Section II briefly reviews Hadoop YARN and defines our system model. Section III presents FSPY scheduler including job size prediction module, job virtual size calculation and scheduling framework. Section IV shows experimental results of our job size prediction module and FSPY scheduler. Finally, we conclude our work in Section V.

% You must have at least 2 lines in the paragraph with the drop letter
% (should never be an issue)

\section{Definition}
\subsection{Jobs and Tasks}
A job is a program instance that is submitted to a Hadoop system. We assume that every job is finite, which means that every job will be finished in a certain time period with enough resources. Jobs that are submitted but not finished yet are called pending jobs. If job J was submitted at time $Release(J)$ and finished at time $Finish(J)$, then the \emph{response time} of \emph{J} can be calculated as follows:

\begin{equation}
Response(J) = Finish(J) - Release(J)
\end{equation}

A \emph(task) is a sub process in a job. Tasks of a job can be executed in parallel. The output of a task generally is a part of either the intermediate results or the final results of its host job. There might also be dependence relationships between tasks in a job.

Tasks in a MapReduce job can be categorized into two subsets: map tasks and reduce tasks. Reduce tasks relies on the output of map tasks in the same job, thus the map tasks are generally launched before the reduce tasks. Therefore, the whole MapReduce job execution process can be divided into the map and the reduce phases. To improve the overall efficiency, there is often some overlapping between the map and the reduce phases in a job.

\subsection{Resource}
The execution of a job requires various types of computing resources. Currently, only two kinds of resource, namely memory and CPU, are scheduled in YARN. The basic unit of resource allocation is called a container. A container contains sufficient computing resources to run an application process (either an AM process or a task process). A container is not a set of concrete hardware, but a logical partition of resources.

When a job is submitted, the central \emph{Resource Manager} (RM) of the cluster will first assign a container to the job to launch its \emph{Application Master} (AM). After the AM is launched, it will schedule the execution of tasks in the job. The AM requests resources from the RM for tasks that are ready to run. The request of an AM contains information about number and resource capacity of its needed containers. In Hadoop, vcore is the basic unit of CPU while memory is measured in MB. Note that the RM scheduler makes no modifications to the resource quantity of requested containers and only determines when to allocate them. After some of the containers are allocated, the AM will further determine how to assign them to specific tasks. After a certain task is completed, its container will be recollected and then rescheduled by the RM.

\subsection{Job size and job virtual size}
We define \emph{task size} as the product of task container resource quantity and its execution time, the \emph{remaining size} of a task as the product of container resource quantity and remaining execution time. Like container resource, the size is also described by a tuple containing both memory and CPU resource quantities. A task¡¯s execution time might differ when running in containers with different hardware and data locality. However, in a certain cluster, the execution time of a task is statistically predictable \cite{ganapathi2010statistics}.

\emph{Job size} equals to the sum of all task sizes in the job. Likewise, the remaining size of a job is sum of all task remaining sizes in the job. Now that the task size is predictable, the job size shall also be predictable.

Suppose there is a virtual cluster that has the same resources with the real cluster but uses Fair scheduling policy. Every job submitted to the real cluster is at the meantime submitted to the virtual cluster. Then, we define \emph{job virtual size} as the virtual remaining size of a job in the virtual cluster.

\subsection{Parallelism bound and average parallelism bound}
Parallelism bound is defined as the most possible resources that a job can get at a certain moment. It is also a tuple containing both memory and CPU resource quantities, which can be calculated as follows:

\begin{equation}
  PB_{i,t}=min({HR_{i,t}+RR_{i,t},CR_t})
\end{equation}
where $PB_{i, t}$ denotes the parallelism bound of $J_i$. $HR_{i, t}$ is the quantity of resources holded by $J_i$. $RR_{i, t}$ represents the quantity of resources that are requested but not acquired by $J_i$. $CR_i$ represents the total resource quantity of the cluster at time $t$.

Even if we ignore the finiteness of the cluster resources, the quantity of resources required by a job might be changing throughout its lifespan. Therefore, the parallelism bound of a job could also be changing all the time. Hence, we introduce \emph{average parallelism bound (APB)} to denote the average resource quantities occupied by the job during its execution under no external resource contention. The \emph{APB} of a job could be calculated as follows:

\begin{equation}
  APB_i=\frac{S_i}{minE_i}
\end{equation}
where $APB_i$ denotes the average parallelism bound of $J_i$. $S_i$  is the size of job $J_i$. $minE_i$ represents the ideal execution time of $J_i$, i.e., the execution time of $J_i$ without external resource competitions.

\begin{sequation}
  minE_i=e_i^m\times ceil\left (\frac{R_i^m\times N_i^m}{CR}  \right )+e_i^r\times ceil\left ( \frac{R_i^r\times N_i^r}{CR} \right )
\end{sequation}
where $R^m_i (R^r_i)$ denotes the container resource requirements of one map (reduce) task in $J_i$. $e^m_i (e^r_i)$ represents the average execution time of map (reduce) tasks in $J_i$. $N^m_i (N^r_i)$ is the count of all map (reduce) tasks in $J_i$. $CR$ stands for the total resource quantity in the cluster. In this paper, we use $APB$ to supplement the calculation of jobs¡¯ virtual sizes.

\subsection{Fairness}
For MapReduce jobs and many other batch processing or interactive analysis applications, response time is a major concern. Hence, we follow the definition of fairness in \cite{friedman2003fairness} that a protocol $P$ is fair if no job completes later under $P$ than under Fair scheduler. According to this definition, Fair scheduler is fair in nature, and FSP is also proved to be fair.

\section{FSPY SCHEDULING ALGORITHM}
FSPY is a size-based scheduler designed for Hadoop YARN. Inspired by FSP, FSPY assigns higher priority to jobs that has smaller virtual size. To track job virtual sizes at each time point, our scheduler uses a new mechanism for job size prediction and a couple of light-weight algorithms for job virtual size calculation. In addition, FSPY adopts a new scheduling framework to integrate these technologies.

\subsection{MapReduce Job Size Prediction}
The programming frameworks on YARN are such different from each other that no one model is capable to predict job sizes for all these frameworks. In this work, we do the prediction for the widely used MapReduce framework. Our prediction method is inspired by \cite{song2013hadoop}, where the job size prediction models are built by job feature analysis and weighted linear regression. Considering the characteristics of YARN, we present a new job features analysis mechanism and a new prediction model.

First of all, we introduce our job size prediction models. The size of a MapReduce job is the sum of all its map and reduce task sizes, as in (5).

\begin{equation}
  S=R_m \times SE_m + R_r \times SE_r
\end{equation}
where $S$ denotes the job size. $R_m (R_r)$ denotes the resource quantity of a map (reduce) task container. $SE_m (SE_r)$ denotes  total execution time of all map (reduce) tasks in the job. $R_m$ and $R_r$ can be obtained from the job's configuration file directly, while $SE_m$ and $SE_r$ should be estimated.

As stated in \cite{song2013hadoop}, the per-task execution time of a MapReduce job can be estimated by a couple of linear models. We make several extensions of the previous models. Firstly, per-task execution time is the prediction object of previous models, while total task execution time in each job phase is the prediction object of our models. This adjustment makes job parallelism considered in the prediction, which can enhance the prediction precision. Secondly, the execution times of map and reduce function (i.e., MFE and RFE) occupy main parts of the execution time of corresponding tasks. The length of these periods can be obtained by our job features analysis mechanism. Hence, they are involved as parameters in our models. Moreover, in general, some overlap exists between map and reduce phases of a job, which makes some early launched reduce tasks have to wait until all map tasks are finished. Therefore, the total execution time of map tasks is involved as parameter in our prediction model for total execution time of reduce tasks. Our prediction models for $SE_m$ and $SE_r$ are stated respectively in (6) and (7).

%\begin{sequation}
%\begin{aligned}
%  SE_m &= \alpha _0 + \alpha _1 \times N_m + \alpha _2 \times MI + \alpha _3 \times MFE + \alpha _4\\& \times MO
%   + \alpha _5 \times MOR + \alpha _6 \times MOR \times logMOR
%\end{aligned}
%\end{sequation}

{\small
\begin{eqnarray}
\begin{aligned}
  SE_m &= \alpha _0 + \alpha _1 \times N_m + \alpha _2 \times MI + \alpha _3 \times MFE + \alpha _4\\& \times MO
   + \alpha _5 \times MOR + \alpha _6 \times MOR \times logMOR
\end{aligned}\\
\cr\noalign{\vskip 2 mm}
\begin{aligned}
  SE_m &= \alpha _0 + \alpha _1 \times N_m + \alpha _2 \times MI + \alpha _3 \times MFE + \alpha _4\\& \times MO
   + \alpha _5 \times MOR + \alpha _6 \times MOR \times logMOR
\end{aligned}
\end{eqnarray}
}%
where $N_m (N_r)$ denotes the number of map (reduce) tasks in the job. $MI$ denotes the total input quantity of map tasks in the job. $MFE (RFE)$ denotes the total execution time of map (reduce) function in the job. $MO (RO)$ denotes the total output quantity of map (reduce) function in the job. $MOR$ denotes the total number of map output records in the job. $RIR$ denotes the total number of reduce input records in the job. ${\alpha _0, \alpha _1, ..., \alpha_6}$ and ${\beta _0, \beta _1, ..., \beta _7}$ are the coefficients. Note that $MFE$ is measured when all the map tasks have their input data in local. In practice, the input data of some map tasks is not stored in local and should be downloaded from HDFS during execution of the map function. With the number of map tasks increasing, the percentage of local map tasks will rise to a certain level and become stable \cite{guo2012investigation}. Therefore, we use $\alpha _2\times MI + \alpha _3\times MFE$ to denote the real total execution time of map function in the job.

Next, we discuss how to apply these models to practice. In prediction models for $SE_m$ and $SE_r$, some parameters (e.g. $MFE$, $RFE$) cannot be obtained directly. To estimate these parameters, we present a new job feature analysis mechanism with probing job. A probing job is a job that executes the same map and reduce functions with the original job but only processes a small fraction of the total input. During the execution of a probing job, job features of our interests (e.g. $MFE$, $RFE$) are collected by the counter mechanism in the MapReduce framework. The counter mechanism provides an approach to track a variety of operations in a MapReduce job. After job features are collected from the probing job, we can calculate corresponding parameters of the original job as follows.

\begin{equation}
  P_o=\frac{P_p}{f_p}
\end{equation}
where $P_o$ denotes a parameter of the original job. $P_p$ denotes the corresponding parameter collected from the probing job. $f_p$ denotes the sampling fraction of input data.

However, the implementation of job probing is faced with many challenges. First of all, probing jobs should consume as less resource and time as possible without reducing the prediction precision. Secondly, the input data of a job may be skewed, which can affect probed parameters \cite{gufler2011handing}. To alleviate the effect of data skew, the input data of probing jobs should be sampled randomly. However, a completely random sampling may be time consuming. Next, the map function execution time can be affected by data locality. In current MapReduce framework, the reading and processing of map function input data are executed in parallel. A map function with input data in local disk generally executes faster than that with input data on other servers.  As our prediction models depend on the map function execution time with data in local, the sampled data should be downloaded to local disk before starting the map function. This means that we need to reconstruct the existing MapReduce framework.

To save resource and time consumed in the probing process, a probing job is set as a tiny job with just one map task and one reduce task in our implementations. Furthermore, we make the probing job run in uber mode in which all processes of a job share just one container. In contrast to running the probing job in normal mode, this setting can reduce the consumed resource and time significantly. If the original job is already in uber mode, we directly estimate its size using historic average task execution time. The uber mode is a marker of the tiny job in MapReduce. In size-based scheduling, the job size prediction precision for tiny jobs has far less influences than that for large jobs. Thus, it is rational to use simpler prediction methods for tiny jobs. Besides, as tiny jobs generally account for a large proportion of the total job set \cite{chen2012interactive}, this strategy can save lots of resources and time.

We present a semi-random sampling algorithm for the probing job, as shown in Algorithm 1. The input parameters of this algorithm include splits (input data splits of the original job), mapperCount (map tasks count of the original job), reducerCount (reduce tasks count of the original job), and sampledSplitsUpperBound (maximum number of splits that can be sampled in this routine). The output is the dataset sampled from the splits. Firstly, we set the sampling fraction to be the reciprocal of the larger number between the counts of map and reduce tasks in the original job. This setting can prevent either the map or reduce task in a probing job from crashing under a too heavy workload. Then, the number of splits to be sampled and the sampling frequency of records in each split are calculated. To reduce consumed time, we do the sampling on a number of splits (not more than sampledSplitsUpperBound) selected randomly from the original splits. Finally, data is sampled from the selected splits according to the sampling frequency.

\begin{algorithm}
  \caption{DATA SAMPLING}
  \KwIn{splits, mapperCount,  reducerCount, sampledSplitsUpperBound}
  \KwOut{sampledData}
  sampleFrac = 1 / max(mapperCount, reducerCount)\;
  sampleSplitNum = min(mapperCount, sampledSplitsUpperBound)\;
  splitSampleFreq = sampleFrac / (sampleSplitNum / mapperCount)\;
  sampledData = an empty collection\;
  \For{$j=0$;$j \le$ mapperCount;$j$ ++}
  {
    $j = random(0,  N^m_i)$\;
    $temSplit = splits[i]$\;
    $splits[i] = splits[j]$\;
    $splits[j] = temSplit$\;
  }
  \For{$j=0$;$j \le$ sampleSplitNum;$j$ ++}
  {
    $sampleBytes = splits[i].length \times splitSampleFreq$\;
    Read $sampleBytes$ of data from $splits[i]$ and append to $sampledData$
  }
  return $sampledData$\;
\end{algorithm}

Moreover, we develop a new version of MapReduce framework, named probing MapReduce, specially for the probing job. In the probing MapReduce framework, the procedure of map task is adjusted and some new counters are added. The map task execution procedure contains the data sampling routine. After the sampling routine is finished, the map function starts to process the data sampled. Native counters in the MapReduce framework cannot collect enough features, we add some new counters to the framework. The new added counters include the counter of map function execution time and the counter of reduce function execution time.

The coefficients in our models can vary in different clusters or different period. Thus, we doesn¡¯t give any fixed values here. These coefficients can be learned by linear regression on history traces. We use generalized least squares \cite{orsini2006generalized} method to do the regression analysis. When a job running under normal mode is finished successfully, its probed parameters (e.g. $MFE, RFE, MO$) and actual parameters (e.g. $SE_m, SE_r$) will be added to the data set for linear regression analysis. After that, the updated models will be used to predict the sizes of subsequent jobs.

\subsection{Job Virtual Size Calculation}
Job virtual size is the most important parameter in our scheduling. The procedure we designed to calculate job virtual size is composed of two algorithms: the Virtual Resource Allocation algorithm (VRA) and the Virtual Size Calculation algorithm (VSC). When a job is submitted or has a virtual size of 0, VSC should be run to update virtual size of each job, and then VRA should be run to update the allocated virtual resource of each job.

According to the principle of Fair scheduler, computing resources should be equally allocated to current jobs. However, in practice, the required resource quantity varies a great deal among different jobs. The fair share may be redundant for some jobs but inadequate for some others. To avoid wastes of resource, the idle resource of some jobs should be reallocated to others whose requirements are not satisfied. Therefore, the actual allocations are not equal among jobs. To calculate the allocation of each job under Fair scheduler, we must consider the differences of jobs¡¯ requirements. As the requirement of a job usually fluctuates during the job's life, it is hard to find out jobs¡¯ requirements at each time point. Instead, we use average parallelism bound (APB) to represent the mean required resource quantity all over the job's life. Furthermore, we calculate job¡¯s fair share in accordance with the dominant resource fairness principle \cite{ghodsi2011dominant}.

For simplicity of description, jobs scheduled by the virtual scheduler is called \emph{virtual jobs}. Jobs that has been submitted but not finished are called \emph{pending virtual jobs}. All pending virtual jobs reside in the \emph{virtual queue}, where jobs are ordered by their APBs. Note that the APB contains two kinds of resources, namely CPU and memory. Ordered by quantities of different resources in the APBs, the order of jobs is commonly different.  According to the common practice in Hadoop \cite{white2012hadoop}, we order the virtual queue by the memory quantities of the APBs.

Algorithm 2 describes the procedure of VRA. The input parameters of this procedure include the virtual queue and the total resource quantity of the cluster. In general, APBs of jobs at the head of the virtual queue would be less than the fair share, while that of jobs at the tail of the queue would be more than the fair share. Virtual resources are allocated to the jobs in order. The unused resources in the fair share of a job will be allocated to its subsequent jobs in the virtual queue. After the execution of this procedure, the virtual resource quantities of each job in the virtual queue are updated.

\begin{algorithm}
  \caption{VIRTUAL RESOURCE ALLOCATION}
  \KwIn{virtualQueue, clusterResource}
  \KwOut{virtualQueue}
  remainingResource = clusterResource\;
  \For{$j=0$;$j\le$ virtualQueue.length;$j$ ++}
  {
    $job = virtualQueue[i]$\;
    $fairShare = remainingResource / (virtualQueue.length-i)$\;
    \eIf{$fairShare > job.APB$}{
        $job.virtualResource = job.APB$\;
    }{
        $feedPercentage = fairShare / job.APB$\;
        $job.virtualResource = job.APB \times feedPercentage$\;
    }
    $remainingResource-=job.virtualResource$\;
  }
  return $virtualQueue$\;
\end{algorithm}

The VSC algorithm depends on the last running result of the VRA algorithm. Algorithm 3 describes the procedure of VSC. The input parameters of VSC include virtual queue and lastUpdateTime, i.e., the last time when VSC was executed. During the interval of two adjacent update, the virtual size variance of a job is the product of its allocated virtual resources and the interval. Thus, current virtual size of the job is the result of its last virtual size subtracting the variance. One thing to note here is that, for jobs not in uber mode, the resources of their AM containers are eliminated from the resources used to calculate the virtual size. As the AM process is in charge of job management rather than doing specific processing works, its execution time is almost equal to the execution time of its host job and can be affected significantly by its competition circumstances and the scheduling policy. Thus, the size of an AM process should not be counted to the job size. After the execution of VSC, virtual sizes of each job in the virtual queue are updated.

\begin{algorithm}
  \caption{VIRTUAL SIZE CALCULATION}
  \KwIn{virtualQueue, lastUpdateTime}
  \KwOut{virtualQueue}
  interval = lastUpdateTime $-$ currentTime\;
  \For{$job$ \textbf{in} $virtualQueue$}
  {
    \If{$job.virtualResource > 0$}{
        \eIf{$job$ is in uber mode}{
            $job.virtualSize -= job.virtualResource \times interval$
        }{
            $effectiveResource = job.virtualResource - job.amResource$
            $job.virtualSize -= effectiveResource \times interval$
        }
        \If{$job.virtualSize <= 0$}{
            remove $job$ from $virtualQueue$
        }
    }
  }
  return $virtualQueue$\;
\end{algorithm}

\subsection{FSPY Scheduling Framework}
We have introduced the techniques replied on by FSPY. To integrate these techniques into FSPY, some problems must be addressed. The first is how to allocate resources among probing and original jobs. To obtain job sizes quickly, probing jobs should be allocated enough resources. However, if too much resources are allocated to probing jobs, the progresses of original jobs might be slowed down. Thus, a proper resource allocation scheme should be established among these jobs. Next, the probing process for job features can be time consuming (usually from a few seconds to tens of seconds). If a job is allocated no resources before its size is predicted, its finish time might be delayed, which can affect the responsiveness and fairness of the scheduler. Thus, how to allocate resources to jobs without size information is a problem.

We present a new scheduling framework to address the problems stated above. In our scheduling framework, cluster resources are allocated to two job queues, named \emph{probing queue} and \emph{production queue}, according to a specified fraction. Probing jobs are placed in the probing queue, while original jobs are placed in the production queue. The resource fraction for each queue can be configured in accordance with specific conditions. Furthermore, the idle resources in one queue can be allocated to the other queue according to the policy of Capacity scheduling. Jobs in the probing queue are scheduled by FIFO. The scheduling policy in the production queue is more sophisticated and will be introduced in the follows. Unless otherwise specified, jobs in the following part of this paper refer to the original jobs rather than the probing jobs.

To address the problem of probing latency, we schedule jobs by the fair policy until their sizes are predicted. Consequently, jobs in the production queue are further arranged into two sub queues: \emph{size-based queue} and \emph{fair queue}. Jobs with size information are placed in the size-based queue and are ordered by their virtual sizes, while jobs without size information are placed in the fair queue and are ordered by the quantities of their allocated resources. Now that both the job virtual size and the allocated resources involve two kinds of resources in YARN, we use memory seconds and memory quantities instead respectively. Moreover, virtual jobs are scheduled in the virtual queue, which has as much virtual resources as the real resources in the cluster. Jobs in the virtual queue are ordered by the memory quantities of their APBs.

As jobs in the production queue are scheduled in two sub queues, how to allocate resources between these two sub queues is a key problem. According to the fair principle, jobs in the fair queue should hold as much resources as they would hold under the virtual Fair scheduler. In other words, their allocated real resources should be as much as their allocated virtual resources. If the fair queue holds less resources than the allocated virtual resources of its jobs, the scheduler will allocate new available resources to the fair queue. Otherwise, new available resources should be allocated to the size-based queue. However, the resources might be rejected by the selected queue for certain reasons, such as the requirement of the queue is fulfilled or the resources don¡¯t meet its requirements. Then, the scheduler will try allocate the resources to the other queue to enhance the resource utilization of the cluster.

The procedure to handle a newly-submitted job is as follows. If the job is running in uber mode, its size is estimated directly as stated above, and then it is put into the size-based queue. If the job is in normal mode, we assign the job an initial size and put it into the fair queue. Then, we start the job probing procedure to predict its job size. Meanwhile, both jobs in uber and normal modes are put into the virtual queue. The initial size of a job should be large enough, otherwise the job may be taken out from the virtual queue by VSC algorithm before it is actually finished in the virtual Fair scheduler. In addition to job size, APB is also necessary in the virtual size calculation. The APB of an uber job is equal to the resource quantity of its AM container, while that of a normal job cannot be calculated until the job size is predicted. Similar to the operation of job size, we assign an initial APB to each normal job before its size is predicted. Now that a job usually runs in the map phase during its probing process, we calculate its initial APB as follows:

\begin{equation}
  APB_{init} = min (R_{am}+R_m \times N_m, RC)
\end{equation}
where $APB_{init}$ denotes the job¡¯s initial APB. $R_{am}$ denotes the resource quantity in the job¡¯s AM container. $R_m$ denotes the resource quantity in a map container of the job. $N_m$ denotes the number of map tasks in the job. $RC$ denotes the total resource quantity in the system.

After the job probing process is finished, job size and APB are obtainted. As the current job virtual size is calculated based on initial job size, it should be updated with the newly-predicted job size. Before calculating the new virtual size, VSC algorithm should be run to update virtual sizes of jobs in the virtual queue. Then, new virtual size of the job is calculated as follows:
\begin{equation}
  VS_{new}=S_{pred}-(S_{init}-VS_{cur})
\end{equation}
where $VS_{new}$ is the new virtual size of the job. $VS_{cur}$ is the current virtual size of the job. $S_{prob}$ is the predicted job size. $S_{init}$ denotes the initial job size. After updating the virtual size, the job is moved from the fair queue to the size-based queue. Finally, the allocated virtual resources of each job in the virtual queue are updated by the VRA algorithm.

When a job is finished, our handling procedure varies by which queue the job belongs to. If the job is in the fair queue, it means that this job is scheduled under fair scheduling principle throughout its lifespan. Then, the corresponding virtual job should be finished at the same time with the real one. As a result, the job should be removed from both the fair queue and the virtual queue. After the job is removed from the virtual queue, the VSC and VRA procedures should be executed in sequence to update the virtual sizes and allocated virtual resources of jobs in the virtual queue. Else, if the job is in the size-based queue, it should only be removed from the size-based queue. Then, the parameters of the job (both real parameters and the probed parameters) are collected to train the prediction models.
%\begin{algorithm}
%\caption{Data Sampling}\label{euclid}
%\begin{algorithmic}[1]
%\Require
%splits, mapperCount,  reducerCount, sampledSplitsUpperBound
%\Ensure
%sampledData
%   \State sampleFrac = 1/ max(mapperCount, reducerCount)
%   \State sampleSplitNum = min(mapperCount, sampledSplitsUpperBound)
%   \State splitSampleFreq = sampleFrac / (sampleSplitNum / mapperCount)
%   \State sampledData = an empty collection
%
%   \If{sfdfdsa}
%   \State sdfsadf
%   \EndIf
%   \While{$r\not=0$}\Comment{We have the answer if r is 0}
%      \State $a\gets b$
%      \State $b\gets r$
%      \State $r\gets a\bmod b$
%   \EndWhile\label{euclidendwhile}
%   \State \textbf{return} $b$\Comment{The gcd is b}
%\end{algorithmic}
%\end{algorithm}

\section{Experiments}
FSPY has been implemented and experimentally assessed. In this section, we provide the experiment details and analyze the results. Our experiments assess the performance of both our job size prediction mechanism and FSPY scheduler as a whole. To evaluate scheduling performance, we compare both fairness and responsiveness of FSPY to the Fair scheduler, which is commonly used in current Hadoop platform. Similar to the relevant works in \cite{pastorelli2015hfsp}, \cite{li2013h}, the FIFO scheduler will not be compared with FSPY in our experiments, as it is demonstrated to be far less efficient than other schedulers.

\subsection{Experimental Setup}
We use three physical servers to build an experimental cluster. A physical server with 8GB RAM and 2 CPU cores is set as the master node of our cluster. The other two servers (each with 32GB RAM and 32 CPU cores) are virtualized to a resource pool by XenCenter. Then, we create 8 virtual servers (each with 8GB virtual RAM and 8 virtual cores) from the resource pool and set them as slave nodes. We deploy the Hadoop platform on our cluster and set the HDFS block size to 64MB.

We use SWIM \cite{chen2011case} to synthesize the workload used in our experiments. SWIM can capture rich workload characteristics observed in traces, and synthesize representative workloads for execution. By default, SWIM provides two formatted workload traces, namely FB-2009 and FB-2010, both of which are sampled and converted from real workload trace files of FaceBook¡¯s datacenter. Comparing to FB-2009, jobs in FB-2010 are larger and submitted in a denser manner. Since the capacity of our experimental cluster is limited, we select FB-2009 to synthesize workloads in our experiments.
\subsection{Job Size Prediction Validation}
Figure \ref{map_execution_time_prediction} compares the predicted and actual per-job map execution times in our experiments. As the figure shows, most jobs are distributed intensively around the diagonal. We also notice that a portion of jobs are located under the diagonal in the lower left corner of the plot, which means that for tiny jobs (per-job map execution time less than 30s),  our predicted values are generally smaller than the actual values. Statistically, the $R^2$ accuracy of our prediction reaches 0.942. The whole results suggest that our method is effective in per-job map execution time prediction, especially for large jobs.
\begin{figure*}
  \centering
  \subfigure[map tasks execution time]{
    \label{map_execution_time_prediction} %% label for first subfigure
    \includegraphics[width=0.32\linewidth]{map_execution_time_prediction}}
  \subfigure[reduce tasks execution time]{
    \label{reduce_execution_time_prediction} %% label for second subfigure
    \includegraphics[width=0.32\linewidth]{reduce_execution_time_prediction}}
  \subfigure[job sizes]{
    \label{jobsize_prediction} %% label for second subfigure
    \includegraphics[width=0.32\linewidth]{jobsize_prediction}}
  \caption{Predicted vs. actual values in the job size prediction.}
  \label{pre_actual_exe_time} %% label for entire figure
\end{figure*}

Figure \ref{reduce_execution_time_prediction} shows the predicted vs. actual total per-job reduce execution times of jobs in our experiments. Apparently, the distribution of points in this figure is less intensive than that in Figure \ref{map_execution_time_prediction}, which means the prediction accuracy for reduce tasks is lower than that for map tasks. Nonetheless, the $R^2$ accuracy of per-job reduce execution time prediction is still up to 0.918. Thus the overall prediction accuracy of reduce task execution time is also quite good.

Figure \ref{jobsize_prediction} compares the predicted and actual job sizes in our verification. The $R^2$ accuracy reaches a high value of 0.970, which exceeds the prediction accuracies of both map and reduce task execution time.  We notice that, generally, the predicted map-task execution times of small jobs are lower than the actual values, while the predicted reduce tasks execution times of small jobs are higher than the actual values. The higher accuracy of job size prediction might result from the offset of prediction deviation of the map and reduce tasks execution time.

\subsection{Validation of FSPY Performance}
Responsiveness and fairness are the core aspects of scheduler performance we are concerned with in this paper. Therefore, these performance aspects of FSPY should be the evaluation focus in our experiments. According to the common practice in scheduling literature, we use job response time as the metric of responsiveness. In our experiments, the average value and empirical cumulative distribution function (ECDF) of job response times under FSPY are compared with that under the Fair scheduler.

While for the evaluation of fairness, there are different metrics with different focuses. One of the commonly used metrics of fairness is job slowdown, i.e., the ratio between job's response time under a specific scheduler and its ideal response time in the system without sharing resources with other jobs. We refer to the commonly used job slowdown as \emph{common slowdown} in this paper. Besides, we also defined another metric, namely \emph{fair slowdown}, to measure the fairness. Now that the fairness definition in this paper is based on the performance of Fair scheduler, the fair slowdown is defined as the ratio between job's response time under a specific scheduler and that under the Fair scheduler. In our experiments, both the common slowdown and fair slowdown are used to evaluate the fairness of FSPY.

As the scheduler performances are affected by the resource competition pressures (RCP) in the system, we should compare the performances of schedulers respectively under different resource competition pressures. In this paper, the RCP of a workload is defined as follows:
\begin{equation}
  RCP = \alpha \times log \left ( \frac{\sum_{i=1}^{n}S_i}{\sum_{i=1}^{n}I_i \times CR} \right ) + \beta
\end{equation}
where $S_i$ denotes job size of $J_i$. $I_i$ is the time interval between submissions of $J_{i-1}$ and $J_i$. $n$ denotes the number of jobs involved in the workload. $CR$ denotes the total resource quantity in the system. $\alpha$ and $\beta$ are coefficients to make the result approximate to an integer.

To get workload with different RCPs, we extract a series of continuous segments from FB-2009 and run the workload built from each segment separately. The FB-2009 is composed of 5894 independent jobs. Each segment contains 200 jobs and adjacent segments have an overlap of 50 jobs. In consequence, we get 37 segments from the FB-2009. Each segment is used to synthesize an executable workload. And then, each synthesized workload is run under the FSPY and Fair scheduler respectively. After the execution, the RCP of each workload, the job response times under the FSPY and Fair scheduler are calculated.

The workloads in our experiments are classified by RCP into 7 groups. Figure \ref{workload-rcp} shows the number of trace segments (workloads) in each RCP group. The group with RCP of 5 has more workloads than all the others. As the RCP rises or falls from 5, the number of workloads in the group decreases. The number of workloads with RCP that is greater than or equal to 6 is up to 19, accounting for more than 50\% of the total.

\subsubsection{Responsiveness}
The comparisons of job average response times under the FSPY and Fair scheduler are given in Figure \ref{group-art}. As shown in the figure, when RCP is less than or equal to 3, the average response time of jobs under the FSPY scheduler and that under Fair scheduler are almost equal. The reason lies in the fact that when the resources are abundant to the workload, every job can get resources timely and the scheduler will have few effects on the performance. When RCP is greater than or equal to 6, the average response time of jobs under the FSPY scheduler is dramatically less than that under the Fair scheduler. Furthermore, as the RCP rises from 6, the gap between FSPY and Fair scheduler widens sharply. Therefore, FSPY is more efficient than the Fair scheduler in terms of job response time, especially when the workload is under a high RCP.
\begin{figure*}
  \centering
  \subfigure[Trace segment numbers in each group.]{
    \label{workload-rcp} %% label for first subfigure
    \includegraphics[width=0.32\linewidth]{workload-rcp}}
  \subfigure[The average job response time in each group.]{
    \label{group-art} %% label for second subfigure
    \includegraphics[width=0.32\linewidth]{group-art}}
  \subfigure[Job response times under RCP of 6.]{
    \label{responsetime-ECDF} %% label for second subfigure
    \includegraphics[width=0.32\linewidth]{responsetime-ECDF}}
  \caption{Workload characteristics and experimental results about responsiveness.}
  \label{pre_actual_exe_time} %% label for entire figure
\end{figure*}

\begin{figure*}
  \centering
  \subfigure[Job common slowdown.]{
    \label{common_slowdown} %% label for first subfigure
    \includegraphics[width=0.32\linewidth]{common_slowdown}}
  \subfigure[Job fair slowdown.]{
    \label{fair_slowdown} %% label for second subfigure
    \includegraphics[width=0.32\linewidth]{fair_slowdown}}
  \subfigure[Job response times.]{
    \label{fair_fspy_response_time} %% label for second subfigure
    \includegraphics[width=0.32\linewidth]{fair_fspy_response_time}}
  \caption{Experimental results about fairness in RCP of 6.}
  \label{pre_actual_exe_time} %% label for entire figure
\end{figure*}

Figure \ref{responsetime-ECDF} shows the ECDF of per-job response times in the group with RCP of 6. The two curves in this figure almost coincide when job response times are less than 15s because that the tiny jobs are generally given the highest priority under both FSPY and the Fair scheduler. In the middle area of the figure, there is a significant gap between the curves of FSPY and Fair scheduler. The ECDF under FSPY exceeds 0.8 at the response time of 40s, which means more than 80\% of jobs under FSPY have a response time below 40s, while the percentage under the Fair scheduler is less than 50\%. It indicates middle-size jobs generally have a shorter response time under FSPY than under the Fair scheduler. Moreover, when job response time exceeds 64s, the two curves coincided again, which means that large jobs have similar response times under these two schedulers.

\subsubsection{Fairness}
We use the common slowdown and fair slowdown as metrics to evaluate the fairness of FSPY. Figure \ref{common_slowdown} shows the ECDF of job common slowdown in the group with RCP of 6. On the whole, the job common slowdown under FSPY is less than that under the Fair scheduler. The percentage of jobs under FSPY with a common slowdown less than 4 exceeds 95\% while it is less than 65\% under the Fair scheduler. Moreover, the maximum of job common slowdown under FSPY (about 5) is significantly less than that under the Fair scheduler (about 10).

Figure \ref{fair_slowdown} shows the ECDF of fair slowdown in the group with RCP of 6. As shown in this figure, jobs with a fair slowdown less than or equal to 1 account for more than 75\% of the total. This indicates that more than 75\% of jobs under FSPY have a response time no longer than that they would have under the Fair scheduler. Besides, more than 98\% of jobs under FSPY have a response time less than 1.5. And the maximum of fair slowdown under FSPY in the experiment approximates 1.7. There are still some jobs whose fair slowdown exceeds 1 under FSPY, which can be attributed to a couple of factors. One is the job size prediction error, which can be reduced but never eliminated. Another is that the job size itself can be fluctuant to certain extent from one run to another.

Figure \ref{fair_fspy_response_time} shows the comparisons of job response times under FSPY and the Fair scheduler. In the figure, most points are below the quadrant diagonal, indicating that most jobs have a shorter response time under FSPY than under the Fair scheduler.  The points above the diagonal are distributed mainly at the bottom-left corner, which reconfirmed the poor precision in the size prediction of small jobs. Although, for small jobs, delay caused by the poor job size prediction precision is relatively short (generally a few seconds), thus can be tolerated in the scheduling. In summary, FSPY, on the whole, can guarantee the fairness of resource scheduling in YARN.
\section{Conclusion}
In this paper, we present the FSPY scheduler to improve the responsiveness of the Hadoop YARN platform. Our scheduler is designed based on the idea of size-based scheduling. We propose a job size prediction mechanism to predict the whole job sizes of MapReduce jobs. Moreover, to guarantee the fairness of scheduling, a couple of novel algorithms are proposed to calculate job virtual sizes. Experimental results show that FSPY has a better responsiveness than Fair scheduler, especially when the system is under a heavy workload. Meanwhile, our scheduler guarantees the fairness between jobs. Furthermore, our job size prediction method also achieves a satisfying precision. In our future work, we will provide job size prediction models for other programming frameworks such as Tez and Spark.


% An example of a floating figure using the graphicx package.
% Note that \label must occur AFTER (or within) \caption.
% For figures, \caption should occur after the \includegraphics.
% Note that IEEEtran v1.7 and later has special internal code that
% is designed to preserve the operation of \label within \caption
% even when the captionsoff option is in effect. However, because
% of issues like this, it may be the safest practice to put all your
% \label just after \caption rather than within \caption{}.
%
% Reminder: the "draftcls" or "draftclsnofoot", not "draft", class
% option should be used if it is desired that the figures are to be
% displayed while in draft mode.
%
%\begin{figure}[!t]
%\centering
%\includegraphics[width=2.5in]{myfigure}
% where an .eps filename suffix will be assumed under latex,
% and a .pdf suffix will be assumed for pdflatex; or what has been declared
% via \DeclareGraphicsExtensions.
%\caption{Simulation Results}
%\label{fig_sim}
%\end{figure}

% Note that IEEE typically puts floats only at the top, even when this
% results in a large percentage of a column being occupied by floats.


% An example of a double column floating figure using two subfigures.
% (The subfig.sty package must be loaded for this to work.)
% The subfigure \label commands are set within each subfloat command, the
% \label for the overall figure must come after \caption.
% \hfil must be used as a separator to get equal spacing.
% The subfigure.sty package works much the same way, except \subfigure is
% used instead of \subfloat.
%
%\begin{figure*}[!t]
%\centerline{\subfloat[Case I]\includegraphics[width=2.5in]{subfigcase1}%
%\label{fig_first_case}}
%\hfil
%\subfloat[Case II]{\includegraphics[width=2.5in]{subfigcase2}%
%\label{fig_second_case}}}
%\caption{Simulation results}
%\label{fig_sim}
%\end{figure*}
%
% Note that often IEEE papers with subfigures do not employ subfigure
% captions (using the optional argument to \subfloat), but instead will
% reference/describe all of them (a), (b), etc., within the main caption.


% An example of a floating table. Note that, for IEEE style tables, the
% \caption command should come BEFORE the table. Table text will default to
% \footnotesize as IEEE normally uses this smaller font for tables.
% The \label must come after \caption as always.
%
%\begin{table}[!t]
%% increase table row spacing, adjust to taste
%\renewcommand{\arraystretch}{1.3}
% if using array.sty, it might be a good idea to tweak the value of
% \extrarowheight as needed to properly center the text within the cells
%\caption{An Example of a Table}
%\label{table_example}
%\centering
%% Some packages, such as MDW tools, offer better commands for making tables
%% than the plain LaTeX2e tabular which is used here.
%\begin{tabular}{|c||c|}
%\hline
%One & Two\\
%\hline
%Three & Four\\
%\hline
%\end{tabular}
%\end{table}


% Note that IEEE does not put floats in the very first column - or typically
% anywhere on the first page for that matter. Also, in-text middle ("here")
% positioning is not used. Most IEEE journals/conferences use top floats
% exclusively. Note that, LaTeX2e, unlike IEEE journals/conferences, places
% footnotes above bottom floats. This can be corrected via the \fnbelowfloat
% command of the stfloats package.



%\section{Conclusion}
%The conclusion goes here. this is more of the conclusion
%
%% conference papers do not normally have an appendix
%
%
%% use section* for acknowledgement
%\section*{Acknowledgment}
%
%
%The authors would like to thank...
%more thanks here


% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://www.ctan.org/tex-archive/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
\bibliography{refs}
%
% <OR> manually copy in the resultant .bbl file
% set second argument of \begin to the number of references
% (used to reserve space for the reference number labels box)




% that's all folks
\end{document}


